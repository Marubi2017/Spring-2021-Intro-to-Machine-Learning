{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Regularization and Hyperparameter Tuning\n",
    "\n",
    "This week, we take a dive into how to improve the performance of our machine learning models. We will focus on regression problems, where we try to predict a numerical value, but the ideas herein are pretty universal across supervised machine learning.\n",
    "\n",
    "First, **regularization** methods sometimes help improve a model's ability to make good predictions on the test set, often at the expense of training accuracy. We focus on some methods developed by mathematician Andrey Tikhonov and used for solving ill-posed inverse problems. Some special cases of his methods and new innovations have become incredibly popular in machine learning.\n",
    "\n",
    "Second, machine learning models we have seen have trainable parameters determined by a learning algorithm, such as the coefficients in linear regression and the shape and prototype parameters in radial basis functions. **Hyperparameters** are numbers we set or decisions we make before running learning algorithms. Wednesday, we focus on effectively making these choices for performance, or **tuning** them.\n",
    "\n",
    "## Lecture 12: Regularization and Overfitting\n",
    "\n",
    "The problem of **overfitting** is an issue where a machine learning model fits too strongly to the training data, which reduces its ability to generalize to make good predictions on the test set. High performance on the test set is typically our most important goal, because this measures how the model performs on data it has not seen before, indicating the model should perform well on real-world data, assuming the test data are representative of the data we hope to predict.\n",
    "\n",
    "Below, we implement ridge regression, which imposes an $L^2$ penalty on the model parameters to minimize the loss function\n",
    "\n",
    "$$L(\\theta) = \\|\\theta_0 + X\\theta - y\\| + \\lambda_2\\|\\theta\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \n",
    "    def __init__(self, alpha = 0.001, lambda2 = 0):\n",
    "        # save variables to object memory\n",
    "        self.alpha = alpha\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, epochs = 1000, update = 100):\n",
    "        # find the dimension of the data\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        # initialize the model parameters\n",
    "        self.theta0 = np.random.uniform()\n",
    "        self.theta = np.random.uniform(size = d)\n",
    "        \n",
    "        # compute the means of X and y\n",
    "        yMean = np.mean(y)\n",
    "        xMean = np.mean(X, axis = 0)\n",
    "        \n",
    "        # standardize the data\n",
    "        X = scale(X)\n",
    "        y = scale(y)\n",
    "        \n",
    "        # compute a scaling parameter\n",
    "        eta = np.linalg.norm(X)\n",
    "        \n",
    "        # train the model\n",
    "        for i in range(epochs):\n",
    "            # compute the predicted y values\n",
    "            predictions = self.theta0 + X @ self.theta\n",
    "            \n",
    "            # compute the error\n",
    "            error = predictions - y\n",
    "            \n",
    "            # compute the sum of squared errors\n",
    "            sse = np.sum(error ** 2)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = sse + self.lambda2 * np.linalg.norm(self.theta) ** 2 #+ self.lambda1 * np.sum(np.abs(self.theta))\n",
    "\n",
    "            # print an update\n",
    "            if (i + 1) % update == 0:\n",
    "                print('Epoch', i + 1, '\\tLoss', loss)\n",
    "                                                \n",
    "            # weight update for the bias\n",
    "            self.theta -= self.alpha * (X.T @ error + 2 * self.lambda2 * self.theta)\n",
    "            self.theta0 = yMean - xMean @ self.theta\n",
    "\n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        # return the predicted outputs\n",
    "        return self.theta0 + X @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \tLoss 27.764469767613814\n",
      "Epoch 20 \tLoss 22.483688892269694\n",
      "Epoch 30 \tLoss 18.82887006999805\n",
      "Epoch 40 \tLoss 16.259606136016274\n",
      "Epoch 50 \tLoss 14.426579060022705\n",
      "Epoch 60 \tLoss 13.100921261733141\n",
      "Epoch 70 \tLoss 12.130460809764411\n",
      "Epoch 80 \tLoss 11.412433525201196\n",
      "Epoch 90 \tLoss 10.876324684721473\n",
      "Epoch 100 \tLoss 10.472975010623673\n",
      "The predicted y values are [2.23941884 2.41970942 2.6        2.78029058 2.96058116]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The theta values are [1.15767536 0.18029058]\n",
      "The r^2 score is 0.42288869088072845\n",
      "The mean squared error is 0.6001957614840425\n",
      "The mean absolute error is 0.6636513034288558\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [10]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "model = RidgeRegression(lambda2 = 10)\n",
    "model.fit(X, y, epochs = 100, update = 10)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions)\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = np.concatenate((np.atleast_1d(model.theta0), model.theta))\n",
    "print('The theta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12: Hyperparameter Tuning and Cross-Validation\n",
    "\n",
    "As our models grow in complexity, we have added a number of hyperparameters to various models, including learning rate, $L^1$ and $L^2$ penalty coefficients, numbers of prototype points in radial basis functions (RBFs) or number of basis functions in linear basis functions (LBFs), and so on, that we need to test models with different hyperparameter settings to see what hyperparameters are effective.\n",
    "\n",
    "However, if we continue with just splitting data into training and test splits and testing all these different models on the same test set, we are undoing the purpose of the train/test split--to test a model on data it has never seen. Here, we are looking at the test set over and over until a model fits well to it, which is not an effective test of performance on new data. Beyond that, we risk overfitting, where the model works well on the data we use, but fails to effectively generalize to truly new data.\n",
    "\n",
    "**Cross-validation** is any of a number of methods for assessing how well a model will generalize to a new data set.\n",
    "\n",
    "The simplest form of cross-validation is to randomly split the data into three subsets:\n",
    "\n",
    "* Training set\n",
    "* Development set (sometimes called the \"dev\" set or validation set)\n",
    "* Testing set\n",
    "\n",
    "And following the procedure for **tuning hyperparameters**:\n",
    "\n",
    "1. Choose some settings for the hyperparameters.\n",
    "2. Train the model on the training set.\n",
    "3. Test the model on the dev set.\n",
    "4. Go back to step 1 until you are satisfied with the hyperparameters.\n",
    "5. Test the model with the hyperparameters chosen on the test set.\n",
    "\n",
    "It is very possible to make this procedure more sophisticated in useful ways by, for example, doing some sort of automated search of the hyperparameter space in steps 1-4. If there are not too many hyperparameters and the model is not computationally expensive to train and test, you can do a grid search via nested loops.\n",
    "\n",
    "This method of cross-validation is recommended when the number of datapoints in the training set is large compared to the dimension of the data. There are no rules set in stone, but a common train/dev/test split is 60\\%/20\\%/20\\% for datasets with hundreds or thousands of datapoints. However, for huge datasets of millions of datapoints, it has been suggested that much smaller dev and test sets are good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
