{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Regularization and Hyperparameter Tuning\n",
    "\n",
    "This week, we take a dive into how to improve the performance of our machine learning models. We will focus on regression problems, where we try to predict a numerical value, but the ideas herein are pretty universal across supervised machine learning.\n",
    "\n",
    "First, **regularization** methods sometimes help improve a model's ability to make good predictions on the test set, often at the expense of training accuracy. We focus on some methods developed by mathematician Andrey Tikhonov and used for solving ill-posed inverse problems. Some special cases of his methods and new innovations have become incredibly popular in machine learning.\n",
    "\n",
    "Second, machine learning models we have seen have trainable parameters determined by a learning algorithm, such as the coefficients in linear regression and the shape and prototype parameters in radial basis functions. **Hyperparameters** are numbers we set or decisions we make before running learning algorithms. Wednesday, we focus on effectively making these choices for performance, or **tuning** them.\n",
    "\n",
    "## Lecture 12: Regularization and Overfitting\n",
    "\n",
    "The problem of **overfitting** is an issue where a machine learning model fits too strongly to the training data, which reduces its ability to generalize to make good predictions on the test set. High performance on the test set is typically our most important goal, because this measures how the model performs on data it has not seen before, indicating the model should perform well on real-world data, assuming the test data are representative of the data we hope to predict.\n",
    "\n",
    "Below, we implement ridge regression, which imposes an $L^2$ penalty on the model parameters to minimize the loss function\n",
    "\n",
    "$$L(\\theta) = \\|\\theta_0 + X\\theta - y\\| + \\lambda_2\\|\\theta\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \n",
    "    def __init__(self, alpha = 0.001, lambda2 = 0):\n",
    "        # save variables to object memory\n",
    "        self.alpha = alpha\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, epochs = 1000, update = 100):\n",
    "        # find the dimension of the data\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        # initialize the model parameters\n",
    "        self.theta0 = np.random.uniform()\n",
    "        self.theta = np.random.uniform(size = d)\n",
    "        \n",
    "        # compute the means of X and y\n",
    "        yMean = np.mean(y)\n",
    "        xMean = np.mean(X, axis = 0)\n",
    "        \n",
    "        # standardize the data\n",
    "        X = scale(X)\n",
    "        y = scale(y)\n",
    "        \n",
    "        # compute a scaling parameter\n",
    "        eta = np.linalg.norm(X)\n",
    "        \n",
    "        # train the model\n",
    "        for i in range(epochs):\n",
    "            # compute the predicted y values\n",
    "            predictions = self.theta0 + X @ self.theta\n",
    "            \n",
    "            # compute the error\n",
    "            error = predictions - y\n",
    "            \n",
    "            # compute the sum of squared errors\n",
    "            sse = np.sum(error ** 2)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = sse + self.lambda2 * np.linalg.norm(self.theta) ** 2 #+ self.lambda1 * np.sum(np.abs(self.theta))\n",
    "\n",
    "            # print an update\n",
    "            if (i + 1) % update == 0:\n",
    "                print('Epoch', i + 1, '\\tLoss', loss)\n",
    "                                                \n",
    "            # weight update for the bias\n",
    "            self.theta -= self.alpha * (X.T @ error + 2 * self.lambda2 * self.theta)\n",
    "            self.theta0 = yMean - xMean @ self.theta\n",
    "\n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        # return the predicted outputs\n",
    "        return self.theta0 + X @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \tLoss 15.05275147027944\n",
      "Epoch 20 \tLoss 7.887071048336132\n",
      "Epoch 30 \tLoss 4.647494921498687\n",
      "Epoch 40 \tLoss 3.5327727503535247\n",
      "Epoch 50 \tLoss 3.5112983932797017\n",
      "Epoch 60 \tLoss 4.003245884168101\n",
      "Epoch 70 \tLoss 4.691697499814393\n",
      "Epoch 80 \tLoss 5.4109096821794385\n",
      "Epoch 90 \tLoss 6.080600023719182\n",
      "Epoch 100 \tLoss 6.667606503805849\n",
      "The predicted y values are [2.14838242 2.37419121 2.6        2.82580879 3.05161758]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The theta values are [0.79352967 0.22580879]\n",
      "The r^2 score is 0.5098898029889929\n",
      "The mean squared error is 0.5097146048914476\n",
      "The mean absolute error is 0.609029450487268\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df3RU9Z3/8efbJEAQCsgPhaCANfwQgSSkIqIWjIiiVkRcdNGNVVexUBUrou1pt7t7etSy/aHVlmW1VdutbLVKWxe//kKqdg/WQCI/RaxiSUB+CgIGSOD9/eNOMslkQiYwyUwur8c5czK5n8/MvLmEF5987v3ca+6OiIi0fSekugAREUkOBbqISEgo0EVEQkKBLiISEgp0EZGQyEzVB/fo0cP79++fqo8XEWmTli1btt3de8ZrS1mg9+/fn5KSklR9vIhIm2RmnzTWpikXEZGQUKCLiISEAl1EJCRSNoceT1VVFeXl5ezfvz/VpUgL6dChA3379iUrKyvVpYiETloFenl5OZ07d6Z///6YWarLkSRzd3bs2EF5eTkDBgxIdTkioZPwlIuZZZhZqZm9GKfNzOwRM/vQzFaYWcHRFLN//366d++uMA8pM6N79+76DUyOWwtLKxjz4GIG3Pe/jHlwMQtLK5L6/s0Zod8JrAW+FKftUiA38hgF/CLytdkU5uGmv185Xi0sreD+51dSWXUIgIpdldz//EoAJuXnJOUzEhqhm1lf4DLg8Ua6XAk87YGlQFcz652UCkVEQmDuy+tqw7xGZdUh5r68LmmfkeiUy0+Be4HDjbTnABvrfF8e2VaPmd1qZiVmVrJt27ZmFZrOPv74Y0aNGkVubi5Tp07l4MGDDfq88cYb5OXl1T46dOjAwoULAVi8eDEFBQWcddZZFBcXU11dXe+17777LhkZGTz33HMJ17Rz507Gjx9Pbm4u48eP57PPPmvQZ+PGjYwbN44hQ4YwdOhQHn744dq22bNnM3jwYIYPH85VV13Frl27attWrFjB6NGjGTp0KMOGDdMUikgCNu2qbNb2o9FkoJvZ5cBWd192pG5xtjW4c4a7z3f3Qncv7Nkz7srVNmnOnDnMmjWL9evX061bN5544okGfcaNG0dZWRllZWUsXryYjh07cvHFF3P48GGKi4tZsGABq1atol+/fjz11FO1rzt06BBz5sxhwoQJcT97yZIl3HjjjQ22P/jggxQVFbF+/XqKiop48MEHG/TJzMzkRz/6EWvXrmXp0qU89thjrFmzBoDx48ezatUqVqxYwcCBA3nggQcAqK6u5vrrr2fevHmsXr2aJUuW6IwVkQT06ZrdrO1HI5ER+hjga2a2AVgAXGhmv4npUw6cWuf7vsCmpFTYir773e/WG6V+5zvf4ZFHHjnia9ydxYsXM2XKFACKi4trR96Nee6557j00kvp2LEjO3bsoH379gwcOBAIgvT3v/99bd+f/exnXH311fTq1atZf5Y//OEPFBcXH7Gm3r17U1AQHL/u3LkzQ4YMoaIiOEhz8cUXk5kZHGI555xzKC8vB+CVV15h+PDhjBgxAoDu3buTkZHBJ598Qm5uLtu3b+fw4cOcf/75vPLKK82qWSTMZk8YRHZWRr1t2VkZzJ4wKGmf0eRBUXe/H7gfwMzGAve4+/Ux3f4IzDSzBQQHQ3e7++Zjquyuu6Cs7JjeooG8PPjpTxttvvnmm5k8eTJ33nknhw8fZsGCBSxevJi8vLy4/X/729/Sq1cvunbtWht+ffv2rQ3FxixYsIC7774bgB49elBVVUVJSQmFhYU899xzbNwYzF5VVFTwwgsvsHjxYt59991m/VG3bNlC797BYYzevXuzdevWI/bfsGEDpaWljBrV8Fj2L3/5S6ZOnQrABx98gJkxYcIEtm3bxrXXXsu9995Lv379mDNnDtOnT2fUqFGceeaZXHzxxc2qWSTMag58zn15HZt2VdKnazazJwxK2gFROIbz0M1sOoC7zwMWAROBD4EvgK8npbpW1r9/f7p3705paSlbtmwhPz+ffv36UXaE/1jiHQs40pkcmzdvZuXKlbVTKGbGggULmDVrFgcOHKg3Mr7rrrt46KGHyMjIaPA+o0aN4sCBA+zdu5edO3fW/qfz0EMPNTo905i9e/dy9dVX89Of/pQvfan+SUw/+MEPyMzMZNq0aUAw5fL222/z7rvv0rFjR4qKihg5ciRFRUXccsstPPvss8ybN++I+0zkeDUpPyepAR6rWYHu7kuAJZHn8+psd2BGMgs70ki6Jd1yyy08+eSTfPrpp9x0003s2bOH888/P27f3/72twwZMoRdu3ZRXV1NZmYm5eXl9OnTp9H3/93vfsdVV11Vb9559OjRvPXWW0AwpfHBBx8AUFJSwrXXXgvA9u3bWbRoEZmZmUyaNIl33nkHCObQn3zySZ588sl6n3PyySezefNmevfuzebNmxudsqmqquLqq69m2rRpTJ48uV7bU089xYsvvsjrr79e+59U3759+epXv0qPHj0AmDhxIsuXL6eoqIgvvviidmpm7969dO7cudH9ICItwN1T8hg5cqTHWrNmTYNtre3AgQM+cOBAHzBggFdXVyf0milTpvgzzzzj7u633XabP/bYY432HTVqlC9evLjeti1btri7+/79+/3CCy/0119/vcHriouL/dlnn22w/Y033vDi4uIG2++55x5/4IEH3N39gQce8NmzZzfoc/jwYb/hhhv8zjvvbND20ksv+ZAhQ3zr1q31tu/cudPz8/N93759XlVV5UVFRf7iiy+6u/vMmTP9Bz/4gf/mN7/xyy67rJE9kB5/zyJtFVDijeSqLs4Vo127dowbN45/+Id/iDvVEc9DDz3Ej3/8Y8444wx27NjBzTffDAQj7FtuuaW234YNG9i4cSNf/epX671+7ty5DBkyhOHDh3PFFVdw4YUXHvOf47777uPVV18lNzeXV199lfvuuw+ATZs2MXHiRAD+8pe/8Otf/7r2OEFeXh6LFi0CYObMmezZs4fx48eTl5fH9OnTAejWrRt33303X/nKV8jLy6OgoIDLLruMP//5z7z77rvMmTOHadOm0a5dO371q18d859DRBJnQeC3vsLCQo+9wcXatWsZMmRISuqpcfjwYQoKCnj22WfJzc1NaS1hlQ5/zyJtlZktc/fCeG0aodexZs0azjjjDIqKihTmItLmpNXVFlPtzDPP5KOPPkp1GSIiR0UjdBGRkFCgi4iEhAJdRCQkFOgiIiGhQK9jx44dtedjn3LKKeTk5NR+H++SuHWVlJRwxx13NPkZ5557blJqXbJkCV26dCE/P59BgwZxwQUX8OKLDW4mFfd1//d//5eUGkQkvegslzq6d+9eew2S73//+3Tq1Il77rmntr1meX88hYWFFBbGPTW0nmSG6fnnn18b4mVlZUyaNIns7GyKiooafc2SJUvo1KlT0v5jEZH00aZH6C19fz6AG2+8kbvvvptx48YxZ84c/vrXv3LuueeSn5/Pueeey7p1wd1GlixZwuWXXw4E/xncdNNNjB07ltNPP73eJXg7depU23/s2LFMmTKFwYMHM23aNGoWeS1atIjBgwdz3nnncccdd9S+75Hk5eXxve99j0cffRSAP/3pT4waNYr8/HwuuugitmzZwoYNG5g3bx4/+clPyMvL46233orbT0TapjY7Qm+N+/PV+OCDD3jttdfIyMjg888/58033yQzM5PXXnuNb3/72/WuX17j/fff54033mDPnj0MGjSI22+/vcGNIEpLS1m9ejV9+vRhzJgx/OUvf6GwsJDbbruNN998kwEDBnDdddclXGdBQQFz584F4LzzzmPp0qWYGY8//jg//OEP+dGPfsT06dPr/ebx2Wefxe0nIm1Pmw30I92fL9mBfs0119Re12X37t0UFxezfv16zIyqqqq4r7nsssto37497du3p1evXmzZsoW+ffvW63P22WfXbsvLy2PDhg106tSJ008/nQEDBgBw3XXXMX/+/ITqrHsZh/LycqZOncrmzZs5ePBg7fvFSrSfiKS/Njvl0hr356tx4okn1j7/7ne/y7hx41i1ahV/+tOfGr2fZvv27WufZ2RkNLhPaGN9juXaOqWlpbXXSPnmN7/JzJkzWblyJf/5n//ZaJ2J9hOR9NdmA7017s8Xz+7du8nJCX4DiL0GeTIMHjyYjz76iA0bNgDwP//zPwm9bsWKFfz7v/87M2bMaFBn3XuUdu7cmT179tR+31g/EWl72mygt8b9+eK59957uf/++xkzZgyHDh1q+gXNlJ2dzc9//nMuueQSzjvvPE4++WS6dOkSt+9bb71Ve9rijBkzeOSRR2rPcPn+97/PNddcw/nnn197MwqAK664ghdeeKH2oGhj/USk7WnTl89dWFrRovfnS5W9e/fSqVMn3J0ZM2aQm5vLrFmzUl1W0ujyuSJH70iXz22zB0Wh5e/Plyr/9V//xVNPPcXBgwfJz8/ntttuS3VJItIGtOlAD6tZs2aFakQuIq0j7ebQUzUFJK1Df78iLSetAr1Dhw7s2LFD/+hDyt3ZsWMHHTp0SHUpIqGUVlMuffv2pby8nG3btqW6FGkhHTp0aLDASkSSI60CPSsrSysVRUSOUpNTLmbWwcz+ambvmdlqM/vXOH3GmtluMyuLPL7XMuWKiEhjEhmhHwAudPe9ZpYFvG1mL7n70ph+b7l705cFFBGRFtFkoHtwhHJv5NusyENHLUVE0kxCZ7mYWYaZlQFbgVfd/Z043UZHpmVeMrOhjbzPrWZWYmYlOvApIpJcCQW6ux9y9zygL3C2mZ0V02U50M/dRwA/AxY28j7z3b3Q3Qt79ux5LHWLiEiMZp2H7u67gCXAJTHbP3f3vZHni4AsM9OVnkREWlEiZ7n0NLOukefZwEXA+zF9TjEzizw/O/K+O5JfroiINCaRs1x6A0+ZWQZBUP/O3V80s+kA7j4PmALcbmbVQCVwrWu5p4hIq0rkLJcVQH6c7fPqPH8UeDS5pYmISHOk1bVcRETk6CnQRURCQoEuIhISCnQRkZBQoIuIhIQCXUQkJBToIiIhoUAXEQkJBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiISEAl1EJCQU6CIiIaFAFxEJCQW6iEhIKNBFREJCgS4iEhIKdBGRkFCgi4iEhAJdRCQkFOgiIq2puhr27m2Rt1agi4i0lAMHYPlyePxxuP12GDUKOneG//iPFvm4zKY6mFkH4E2gfaT/c+7+LzF9DHgYmAh8Adzo7suTX65Iy1pYWsHcl9exaVclfbpmM3vCICbl56S6rLSl/VXHF1/AihVBgNc8Vq2Cqqqg/UtfgoIC+MY3YNy4FimhyUAHDgAXuvteM8sC3jazl9x9aZ0+lwK5kcco4BeRryJtxsLSCu5/fiWVVYcAqNhVyf3PrwQ4fkPqCI7r/bVnD5SVBaG9bFnwde1aOHw4aO/ePQjvb30r+JqfD6efDie07KRIk4Hu7g7UTPhkRR4e0+1K4OlI36Vm1tXMerv75qRWK9KC5r68rjacalRWHWLuy+vCH1BH4bjZXzt3Qmlp/ZH3+vXgkRg85RQYORImTw6Ce+RIOPVUMGv1UhMZoWNmGcAy4AzgMXd/J6ZLDrCxzvflkW31At3MbgVuBTjttNOOsmSRlrFpV2Wzth/vQrm/tmwJwrtm1L18OWzYEG3v1y8YcV9/fRDc+fnQu3fKyo2VUKC7+yEgz8y6Ai+Y2VnuvqpOl3j/FcWO4nH3+cB8gMLCwgbtIqnUp2s2FXHCqE/X7BRUk/7a9P5yh/Ly+qPu5cth06Zon9xcOPvs4GBmQQHk5UGPHqmrOQEJBXoNd99lZkuAS4C6gV4OnFrn+75AnT0jkv5mTxhUb04YIDsrg9kTBqWwqvTVZvaXO3z8cf1R9/LlsH170H7CCTB4MBQVRee78/ODg5htTCJnufQEqiJhng1cBDwU0+2PwEwzW0BwMHS35s+lramZ99VZG4lJy/116FAwvx078t69O2jPzISzzoKvfS063z1iBHTsmLqak8jcjzzzYWbDgaeADILz1n/n7v9mZtMB3H1e5LTFRwlG7l8AX3f3kiO9b2FhoZeUHLGLiEjjqqthzZr6wV1WBvv2Be3t2wdhXRPcBQVBmLdvn9q6j5GZLXP3wnhtiZzlsgLIj7N9Xp3nDsw4liJFRBp14EBwTnfd8F6xAvbvD9pPPDGY477ppiC4R44MplGyslJbdytr1hy6iEiL27cv/gKd6uqgvUuX6AKdmpF3bi5kZKS27jSgQBeR1Nm9O7pAp+bx/vvRBTo9ewZTJrNn11+gk4JzvNsCBbqItI4dOxoerPzww2h7nz7BiPuaa6Lz3jk5Cu9mUKCLSPJ9+mn94F62DP7+92j7gAHBiPvGG6MLdE4+OWXlhoUCXUSOnjts3Nhw5L25zlnLAwfC6NEwc2Z02uSkk1JXc4gp0EUkMe7w0Uf1R93LlwdTKRAs0BkyBMaPjwZ3Xl6bXKDTVinQRaShQ4dg3bogsGsuTFVaGl2gk5UVnNM9aVJ0vnv48NAs0GmrFOgix7uqqugCnZpR93vvBdf3BujQIVig84//GB15h2CBThgp0EWOJ/v3w8qVDRfoHDwYtHfqFAT2P/9zdOQ9eHCwZF7Snv6WRMJq375gpF135L16dTCdAtCtWzDivuOO6AKdM85o8ZswSMtRoIuEwe7dDW/C8P770Zsw9OwZBPbllwdfCwqgf3+d4x0yCnSRtmb79oY3Yfjb36LtOTnBiHvq1Oi0SZ8+Cu/jgAJdJF25B+dz14R2zQi87gKd008PQvvrX49Om/TqlbqaJaUU6CLpwD0I6tibMGzZErSbwaBBMGZMMOddcwedbt1SW7ekFQW6SGs7fDiYIoldXblzZ9CekREs0Lnkkuiy+Ly84AwUkSNQoIu0pJoFOnVH3qWlsGdP0N6uHQwbFtwxvmbKZNgwyG4D9+WUtKNAF0mWgwfj30GnMnIj5ezsYDXlDTdER95DhwahLpIECnSRo1FZ2XCBzsqV0QU6nTsHgX3bbdEzTQYN0gIdaVH66RJpyp490QU6NY81a6ILdE46KZgqufPO6LTJl7+sBTrS6hToInV99lnDBToffBBdoHPyyUFgX3ll9Lom/frpHG9JCwp0OX5t3RoN75qDlh9/HG0/9dRgxD1tWnTapHfv1NUr0gQFuoSfO2za1PA0wfLyaJ8vfxkKC4OLUtUcsOzZM3U1ixwFBbqEizt88knDmzBs3Rq0mwVXD7zgguh8d14edO2a2rpFkkCBLm3X4cPBTYZjR96ffRa0Z2QEpwVOnBidMhkxQgt0JLSaDHQzOxV4GjgFOAzMd/eHY/qMBf4A1ExAPu/u/5bcUuW4Vl0dXD2w7qi7rAz27g3a27ULzvGeMqX+Ap0OHVJbt0grSmSEXg18y92Xm1lnYJmZverua2L6veXulye/RDnuHDgQXLe77qj7vfeCmzNAcJuzvDwoLq6/QCcrK7V1i6RYk4Hu7puBzZHne8xsLZADxAa6SPNVVgZ3zKm7NH7VquC2aBDcYLigAL7xjei0ycCBwXSKiNTTrDl0M+sP5APvxGkebWbvAZuAe9x9dZzX3wrcCnDaaac1t1Zp6/bsCaZJ6o68166NLtDp3j0I71mzotMmp5+uBToiCUo40M2sE/B74C53/zymeTnQz933mtlEYCGQG/se7j4fmA9QWFjoR121pL+aBTp1R97r10cX6PTuHYy4r7oqukDntNO0QEfkGCQU6GaWRRDm/+3uz8e21w14d19kZj83sx7uvj15pUra2rq14Zkm8RboXH999PZnWqAjknSJnOViwBPAWnf/cSN9TgG2uLub2dnACcCOpFYqqecOFRUNw7uiItrnjDPgK1+BW2+NHrDs0SN1NYscRxIZoY8BbgBWmllZZNu3gdMA3H0eMAW43cyqgUrgWnfXlEpb5h6MsmPDe9u2oP2EE4IFOmPHBsFdc453ly4pLVvkeJbIWS5vA0ec2HT3R4FHk1WUtLJDh4L57brBXVoKu3YF7ZmZcNZZcMUV0fnuESPgxBNTW7eI1KOVoseb6urgzJK6ByvLymDfvqC9fftggc7UqdEzTc46K9guImlNgR5mBw4E53TXHXmvWBFdoHPiicFo+6aboiPvM8/UAh2RNkqBHhZffBGEdd2LUq1eHV2g06VLENozZkQX6OTmaoGOSIgo0Nuizz+Pv0Dn8OGgvUePILBr7hpfUAADBugcb5GQU6Cnux07Gt5BZ/36aHufPsGI++qro+d49+2r8BY5DinQ08mnnzY8TfCTT6Lt/foFI+7i4iDECwrglFNSV6+IpBUFeiq4B3fLib0Jw+bN0T4DB8Lo0cFFqWoW6Jx0UupqFpG0p0Bvae7w0UcNR97bI1dFOOEEGDIELrooOt89YkRwlUERkWZQoCfToUPBHeJjF+js3h20Z2UF53RfeWX0TJPhw4Pre4uIHCMF+tGqqoI1a+qHd1lZcPogBHfKGTECrrsuOvIeOlQLdESkxSjQE7F/f7BAp+7qypUrg4U7ENyjMi8vuGN8zQKdIUOCJfMiIq1EiRNr377gdmd1R96rVwdL5iG4O3xBAXzzm8HXkSODKwzqJgwikmLHd6Dv3t3wHO/334/ehKFnzyCwJ06MTpv0769zvEUkLR0/gb59e/3wXrYM/va3aHtOThDYU6dGp01ychTeItJmhDPQN22qf5bJ8uXw979H2/v3D0bcN90UXaBz8skpK1dEJBnadqC7B0Ede473p58G7WbBAp1zz43OeefnQ7duqa1bRKQFtL1AX7UKfv3raHjv3BlsP+GE4NKvEyZEgzsvDzp3Tm29IiKtpO0F+scfw09+AsOGweTJ0QU6w4ZpgY6IHNfaXqBPmAB790K7dqmuREQkrbS9QFeQi4jEpdUwIiIhoUAXEQkJBbqISEgo0EVEQqLJg6JmdirwNHAKcBiY7+4Px/Qx4GFgIvAFcKO7L09+udJcC0srmPvyOjbtqqRP12xmTxjEpPycVJclIi0gkbNcqoFvuftyM+sMLDOzV919TZ0+lwK5kcco4BeRr5JCC0sruP/5lVRWHQKgYlcl9z+/EkChLhJCTU65uPvmmtG2u+8B1gKxaXAl8LQHlgJdzax30quVZpn78rraMK9RWXWIuS+vS1FFItKSmjWHbmb9gXzgnZimHGBjne/LaRj6mNmtZlZiZiXbtm1rXqXSbJt2VTZru4i0bQkHupl1An4P3OXun8c2x3mJN9jgPt/dC929sGfPns2rVJqtT9fsZm0XkbYtoUA3syyCMP9vd38+Tpdy4NQ63/cFNh17eXIsZk8YRHZWRr1t2VkZzJ4wKEUViUhLajLQI2ewPAGsdfcfN9Ltj8A/WeAcYLe7b05inXIUJuXn8MDkYeR0zcaAnK7ZPDB5mA6IioRUIme5jAFuAFaaWVlk27eB0wDcfR6wiOCUxQ8JTlv8evJLlaMxKT9HAS5ynGgy0N39beLPkdft48CMZBUlIiLNp5WiIiIhoUAXEQkJBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiISEAl1EJCQU6CIiIaFAFxEJCQW6iEhIKNBFREJCgS4iEhIKdBGRkFCgi4iEhAJdRCQkFOgiIiGhQBcRCQkFuohISCjQRURCQoEuIhISCnQRkZBQoIuIhIQCXUQkJJoMdDP7pZltNbNVjbSPNbPdZlYWeXwv+WWKiEhTMhPo8yTwKPD0Efq85e6XJ6UiERE5Kk2O0N39TWBnK9QiIiLHIFlz6KPN7D0ze8nMhjbWycxuNbMSMyvZtm1bkj5aREQgOYG+HOjn7iOAnwELG+vo7vPdvdDdC3v27JmEjxYRkRrHHOju/rm77408XwRkmVmPY65MRESa5ZgD3cxOMTOLPD878p47jvV9RUSkeZo8y8XMngHGAj3MrBz4FyALwN3nAVOA282sGqgErnV3b7GKRUQkriYD3d2va6L9UYLTGkVEJIW0UlREJCQU6CIiIaFAFxEJCQW6iEhIKNBFREJCgS4iEhIKdBGRkFCgi4iEhAJdRCQkFOgiIiGhQBcRCQkFuohISCjQRURCQoEuIhISCnQRkZBQoIuIhIQCXUQkJBToIiIhoUAXEQkJBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiIREk4FuZr80s61mtqqRdjOzR8zsQzNbYWYFyS8zsLC0gjEPLmbAff/LmAcXs7C0oqU+SkSkzUlkhP4kcMkR2i8FciOPW4FfHHtZDS0sreD+51dSsasSByp2VXL/8ysV6iIiEU0Guru/Cew8Qpcrgac9sBToama9k1Vgjbkvr6Oy6lC9bZVVh5j78rpkf5SISJuUjDn0HGBjne/LI9saMLNbzazEzEq2bdvWrA/ZtKuyWdtFRI43yQh0i7PN43V09/nuXujuhT179mzWh/Tpmt2s7SIix5tkBHo5cGqd7/sCm5LwvvXMnjCI7KyMetuyszKYPWFQsj9KRKRNSkag/xH4p8jZLucAu919cxLet55J+Tk8MHkYOV2zMSCnazYPTB7GpPy4szsiIsedzKY6mNkzwFigh5mVA/8CZAG4+zxgETAR+BD4Avh6SxU7KT9HAS4i0ogmA93dr2ui3YEZSatIRESOilaKioiEhAJdRCQkFOgiIiGhQBcRCQkLjmmm4IPNtgGfHOXLewDbk1hOsqRrXZC+tamu5lFdzRPGuvq5e9yVmSkL9GNhZiXuXpjqOmKla12QvrWpruZRXc1zvNWlKRcRkZBQoIuIhERbDfT5qS6gEelaF6RvbaqreVRX8xxXdbXJOXQREWmorY7QRUQkhgJdRCQk0jrQzayrmT1nZu+b2VozGx3T3mo3qG5mXWPNbLeZlUUe32uFmgbV+bwyM/vczO6K6dPq+yvBulp9f0U+d5aZrTazVWb2jJl1iGlP1c9XU3Wlan/dGalpdezfYaQ9VfurqbpabX+Z2S/NbKuZraqz7SQze9XM1ke+dmvktZeY2brI/rvvqApw97R9AE8Bt0SetwO6xrRPBF4iuGvSOcA7aVLXWODFFO63DOBTggUIKd9fCdTV6vuL4DaJHwPZkSCr264AAANBSURBVO9/B9yY6v2VYF2p2F9nAauAjgRXaX0NyE2D/ZVIXa22v4ALgAJgVZ1tPwTuizy/D3gozusygL8Bp0cy5T3gzOZ+ftqO0M3sSwQ75wkAdz/o7rtiurXKDaqPoq5UKwL+5u6xK3FbfX8lWFeqZALZZpZJEAixd9pK1f5qqq5UGAIsdfcv3L0a+DNwVUyfVOyvROpqNe7+JrAzZvOVBINAIl8nxXnp2cCH7v6Rux8EFkRe1yxpG+gE/1NtA35lZqVm9riZnRjTJ+EbVLdyXQCjzew9M3vJzIa2cE2xrgWeibM9FfurrsbqglbeX+5eAfwH8HdgM8Gdtl6J6dbq+yvBuqD1f75WAReYWXcz60gwGj81pk8qfr4SqQtS++/xZI/cxS3ytVecPknZd+kc6JkEv7r8wt3zgX0Ev67UlfANqlu5ruUE0wojgJ8BC1u4plpm1g74GvBsvOY421rlvNUm6mr1/RWZx7wSGAD0AU40s+tju8V5aYvurwTravX95e5rgYeAV4H/RzAlUB3TrdX3V4J1pezfYzMkZd+lc6CXA+Xu/k7k++cIgjS2T4vfoLq5dbn75+6+N/J8EZBlZj1auK4alwLL3X1LnLZU7K8ajdaVov11EfCxu29z9yrgeeDcmD6p2F9N1pWqny93f8LdC9z9AoJphfUxXVLy89VUXSn+9wiwpWbqKfJ1a5w+Sdl3aRvo7v4psNHMBkU2FQFrYrq1yg2qm1uXmZ1iZhZ5fjbBft7RknXVcR2NT2u0+v5KpK4U7a+/A+eYWcfIZxcBa2P6pGJ/NVlXqn6+zKxX5OtpwGQa/n2m5OerqbpS/O8Rgv1SHHleDPwhTp93gVwzGxD5bfbayOuap6WP+h7LA8gDSoAVBL8mdQOmA9Mj7QY8RnB0eCVQmCZ1zQRWE/z6txQ4t5Xq6kjwg9qlzrZ02F9N1ZWq/fWvwPsE87C/Btqnyf5qqq5U7a+3CAYv7wFFafTz1VRdrba/CP4z2QxUEYy6bwa6A68T/ObwOnBSpG8fYFGd104EPojsv+8czedr6b+ISEik7ZSLiIg0jwJdRCQkFOgiIiGhQBcRCQkFuohISCjQRURCQoEuIhIS/x9/EwOokhMbkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [10]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "model = RidgeRegression(lambda2 = 10)\n",
    "model.fit(X, y, epochs = 100, update = 10)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions)\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = np.concatenate((np.atleast_1d(model.theta0), model.theta))\n",
    "print('The theta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12: Hyperparameter Tuning and Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
