{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Regularization and Hyperparameter Tuning\n",
    "\n",
    "This week, we take a dive into how to improve the performance of our machine learning models. We will focus on regression problems, where we try to predict a numerical value, but the ideas herein are pretty universal across supervised machine learning.\n",
    "\n",
    "First, **regularization** methods sometimes help improve a model's ability to make good predictions on the test set, often at the expense of training accuracy. We focus on some methods developed by mathematician Andrey Tikhonov and used for solving ill-posed inverse problems. Some special cases of his methods and new innovations have become incredibly popular in machine learning.\n",
    "\n",
    "Second, machine learning models we have seen have trainable parameters determined by a learning algorithm, such as the coefficients in linear regression and the shape and prototype parameters in radial basis functions. **Hyperparameters** are numbers we set or decisions we make before running learning algorithms. Wednesday, we focus on effectively making these choices for performance, or **tuning** them.\n",
    "\n",
    "## Lecture 12: Regularization and Overfitting\n",
    "\n",
    "The problem of **overfitting** is an issue where a machine learning model fits too strongly to the training data, which reduces its ability to generalize to make good predictions on the test set. High performance on the test set is typically our most important goal, because this measures how the model performs on data it has not seen before, indicating the model should perform well on real-world data, assuming the test data are representative of the data we hope to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-d399e1ee357c>, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-d399e1ee357c>\"\u001b[1;36m, line \u001b[1;32m75\u001b[0m\n\u001b[1;33m    def coordinate_descent_lasso(self):\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class RegularizedLinearRegression:\n",
    "    \n",
    "    def __init__(self, alpha = 0.001, lambda1 = 0, lambda2 = 0):\n",
    "        # save variables to object memory\n",
    "        self.alpha = alpha\n",
    "        #self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, epochs = 1000, update = 100):\n",
    "        # find the dimension of the data\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        # initialize the model parameters\n",
    "        self.theta0 = np.random.uniform()\n",
    "        self.theta = np.random.uniform(size = d)\n",
    "        \n",
    "        # compute the means of X and y\n",
    "        yMean = np.mean(y)\n",
    "        xMean = np.mean(X, axis = 0)\n",
    "        \n",
    "        # standardize the data\n",
    "        X = scale(X)\n",
    "        y = scale(y)\n",
    "        \n",
    "        # compute a scaling parameter\n",
    "        eta = np.linalg.norm(X)\n",
    "        \n",
    "        # train the model\n",
    "        for i in range(epochs):\n",
    "            # compute the predicted y values\n",
    "            predictions = self.theta0 + X @ self.theta\n",
    "            \n",
    "            # compute the error\n",
    "            error = predictions - y\n",
    "            \n",
    "            # compute the sum of squared errors\n",
    "            sse = np.sum(error ** 2)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = sse + self.lambda2 * np.linalg.norm(self.theta) ** 2 #+ self.lambda1 * np.sum(np.abs(self.theta))\n",
    "\n",
    "            # print an update\n",
    "            if i % update == 0:\n",
    "                print('Epoch', i, '\\tLoss', loss)\n",
    "                                \n",
    "            # weight update for the bias\n",
    "            self.theta -= self.alpha * (X.T @ error + 2 * self.lambda2 * self.theta)\n",
    "            self.theta0 = yMean - xMean @ self.theta\n",
    "\n",
    "            # weight update for theta (where soft-threholding follows)\n",
    "            #print(error)\n",
    "            #print(X.T @ error)\n",
    "            #print(np.sign(self.theta))\n",
    "            #print(eta * (X.T @ error + 2 * self.lambda2 * self.theta + self.lambda1 * np.sign(self.theta)))\n",
    "            #self.theta -= eta * (X.T @ error + 2 * self.lambda2 * self.theta + self.lambda1 * np.sign(self.theta))\n",
    "            \n",
    "            # soft-threholding for the LASSO            \n",
    "            #self.theta = np.sign(self.theta) * np.maximum(np.zeros([1, d]), np.abs(self.theta) - self.alpha * eta)\n",
    "\n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        # return the predicted outputs\n",
    "        return self.theta0 + X @ self.theta\n",
    "    \n",
    "    # soft-threshold function for LASSO with normalized data\n",
    "    def soft_threshold(rho, lamda):\n",
    "        if rho < -lamda:\n",
    "            return (rho + lamda)\n",
    "        elif rho > lamda:\n",
    "            return (rho - lamda)\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def coordinate_descent_lasso(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7a3262bfc975>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegularizedLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambda2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [10]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "model = RegularizedLinearRegression(lambda2 = 0.1)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions)\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = np.concatenate((np.atleast_1d(model.theta0), model.theta))\n",
    "print('The theta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12: Hyperparameter Tuning and Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
