{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Numerical Optimization\n",
    "\n",
    "## Lecture 9 (Mon, Feb 1) Implementing Gradient Descent for Regression\n",
    "\n",
    "Last time, we learned about the idea of gradient descent and the related mathematics (see the Lecture 8 notes in Canvas for details). Today, we will focus on implementing gradient descent using basic `NumPy`. Gradient descent is a general-purpose optimization algorithm, but we will start by using it for some regression problems using some methods we have developed in the class so far.\n",
    "\n",
    "### Some Ideas from Multivariate Calculus\n",
    "\n",
    "First, we need just a few ideas from multivariate calculus. This is quite minimal, but you learn more details about these topics in sections 4.3 (partial derivatives), 4.6 (gradients), and 4.7 (multivariate optimization) of <a href=\"https://openstax.org/details/books/calculus-volume-3\">*Calculus Volume 3*</a> by Strang.\n",
    "\n",
    "The ideas we need for gradient descent include:\n",
    "\n",
    "* If we have a differentiable function of several variables, like our loss function $L(\\theta) = L(\\theta_0, ..., \\theta_d)$, we can define the **partial derivatives** with respect to each of these variables as\n",
    "\n",
    "    $$L_{\\theta_i}=\\frac{\\partial L}{\\partial \\theta_i}=\\lim\\limits_{h\\to 0}\\frac{L(\\theta+he_i) - L(\\theta)}{h},$$\n",
    "\n",
    "    where $e_i$ is a $(d+1)$-vector with all 0s except for a 1 in the $i$th component.  Geometrically, this partial derivative is the slope of $L$ if we go in the direction of $e_i$.\n",
    "    \n",
    "* To **minimize a multivariable function** by hand, we need to find critical points, which are points $\\theta$ where *all* partial derivatives are 0 and compare which ones give the lowest outputs. In numerical algorithms, must settle for approximations that are \"nearly\" critical points.\n",
    "    \n",
    "* If we put these partial derivatives into a vector of $d+1$ variables, we call that a **gradient**, which we denote\n",
    "    \n",
    "    $$\n",
    "    \\nabla L(\\theta)\n",
    "    =\\begin{pmatrix}\n",
    "    L_{\\theta_0}(\\theta) \\\\\n",
    "    \\vdots \\\\\n",
    "    L_{\\theta_d}(\\theta)\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    \n",
    "* The **directional derivative** of a function in the direction of a unit vector $u$ starting from a point $\\theta$.\n",
    "\n",
    "$$D_u L(\\theta) = \\lim\\limits_{h\\to 0}\\frac{L(\\theta+hu) - L(\\theta)}{h},$$\n",
    "    \n",
    "which is the slope in the direction of the vector $u$. Since $L$ is differentiable, this directional derivative will be defined for all directions leaving from $\\theta$. In 1D, there are just two directions: left of right. In 2D, we have directional derivatives at every angle in a circle around the point.\n",
    "\n",
    "* A common theorem says the **directional derivative is maximized in the direction of the gradient** at each point, so the gradient gives the direction of the *steepest ascent* in the function $L$. Similarly, the direction of the *steepest descent* is $-\\nabla L(\\theta)$, the opposite direction.\n",
    "\n",
    "### The Geometry of Gradient Descent\n",
    "\n",
    "We will discuss the geometry of gradient-based methods in class, but let's discuss a general outline of how gradient descent works, setting aside the stochastic version for now. The goal of gradient descent is to approximately solve the minimization problem\n",
    "\n",
    "$$\\min\\limits_{\\theta}\\,L(\\theta)$$\n",
    "\n",
    "by finding (approximate) critical values by making a guess for the location of a critical value, taking a small step in the opposite direction as the gradient, and repeating this over and over until, hopefully, we reach a minimum value.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "0. Make a guess for the critical value -- $\\theta^0$\n",
    "1. Compute the gradient of $L$ at $\\theta^0$\n",
    "2. Take a small step to $\\theta^1 = \\theta^0 - \\alpha\\nabla L\\left(\\theta^0\\right)$\n",
    "3. Compute the gradient of $L$ at $\\theta^1$\n",
    "4. Take a small step to $\\theta^2 = \\theta^1 - \\alpha\\nabla L\\left(\\theta^1\\right)$\n",
    "5. (repeat until the gradient gets close to $(0, ..., 0)$)\n",
    "\n",
    "This $\\alpha>0$ is a number that will be used in the algorithm as a multiplier of the steps the method will take. This is called the **learning rate**.\n",
    "\n",
    "This idea seems plausible from the calculus ideas above because we just keep switching directions and making a step in the direction of the steepest downward path--the opposite direction as the gradient--until we reach a good place. This is a \"greedy\" algorithm because it just picks the quickest step in each iteration, which is fast, but it is likely to land in the first minimum it finds, which may or may not be optimal.\n",
    "\n",
    "If you had two parameters, $L$ would be like a 3D curved surface. A nice visual to have in mind is a rain drop falling on a huge leaf. The droplet of water will move in the steepest downward direction due to gravity--but this direction *changes* as the drip follows the contours of the leaf. This is what gradient descent does.\n",
    "\n",
    "Will the drip land in the physically lowest altitude part of the leaf? Maybe, but maybe not. If the rain drop lands on the edge, it will probably just roll off the edge. If the leaf has a few different \"sinks,\" different initial locations of the rain drop might cause it to land in these different ones, some of which have lower altitudes than others. Now, if there is heavy rain and lots of rain drops land on the leaf, we can be pretty sure *some* of them will reach the lowest-altitude sink.\n",
    "\n",
    "From this analogy, you might get the idea that we can make several initial guesses and run it to be more confident we will find the global minimum and not just a local minimum.\n",
    "\n",
    "In the end, if some of our initial guesses are good choices, the step size $\\alpha$ is not too big or too small, and the loss function is pretty well-behaved, the method will converge approximately to a local minimum.\n",
    "\n",
    "### Implementing Gradient Descent\n",
    "\n",
    "Let's import some libraries then write a function for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent will need a few inputs:\n",
    "\n",
    "* A function to minimize $f$\n",
    "* A starting point $x_0$\n",
    "* A learning rate $\\alpha$\n",
    "* A small number $h$ (the variable that goes to 0 in the definition of the derivative)\n",
    "* A small positive value that we can use for a stopping condition for the derivative being sufficiently small (the tolerance)\n",
    "* A maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradient(f, x, h):\n",
    "    n = len(x)\n",
    "    gradient = np.zeros(n)\n",
    "    \n",
    "    for counter in range(n):\n",
    "        xUp = x.copy()\n",
    "        xUp[counter] += h\n",
    "        gradient[counter] = (f(xUp) - f(x))/h\n",
    "            \n",
    "    return gradient\n",
    "\n",
    "# run gradient descent ant output the \n",
    "def gradientDescent(f, x0, alpha, h, tolerance, maxIterations):\n",
    "    # set x equal to the initial guess\n",
    "    x = x0\n",
    "                \n",
    "    # take up to maxIterations number of steps\n",
    "    for counter in range(maxIterations):\n",
    "        # update the gradient\n",
    "        gradient = computeGradient(f, x, h)\n",
    "        \n",
    "        # stop if the norm of the gradient is near 0\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            #print('Gradient descent took', counter, 'iterations to converge')\n",
    "            #print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "            # return the approximate critical value x\n",
    "            return x\n",
    "        \n",
    "        # if we do not converge, print a message\n",
    "        elif counter == maxIterations-1:\n",
    "            #print(\"Gradient descent failed\")\n",
    "            #print('The gradient is', gradient)\n",
    "            # return x, sometimes it is still pretty good\n",
    "            return x\n",
    "        \n",
    "        # take a step in the opposite direction as the gradient\n",
    "        x -= alpha*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS by Gradient Descent\n",
    "\n",
    "Ideally, we want to use gradient descent for radial basis function expansions, for which we were unable to find an explicit formula for model parameters because the loss function does not have a unique solution, but let's start with a simpler problem: finding an approximate solution for the ordinary least squares problem.\n",
    "\n",
    "Recall that, in this situation, we have some labeled datapoints $(x_{i1}, ..., x_{id}, y_i)$, where we fit a function of the form\n",
    "\n",
    "$$f_\\theta(x_i) = \\theta_0 + \\sum\\limits_{j=1}^d\\theta_jx_{ij}$$\n",
    "\n",
    "To find the least squares solution, we aim to solve the minimization problem $\\min\\limits_\\theta L(\\theta)$ for $\\theta=(\\theta_0, \\theta_1, ..., \\theta_d)\\in\\mathbb{R}^{d+1}$, where the loss function is\n",
    "\n",
    "$$L(\\theta)=\\sum\\limits_{i=1}^n \\left(y_i - f_\\theta(x_i)\\right)^2=\\sum\\limits_{i=1}^n \\left(y_i - \\theta_0 - \\sum\\limits_{j=1}^d\\theta_jx_{ij}\\right)^2$$\n",
    "\n",
    "Let's bring in our exact OLS class for linear regression from Week 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquaresExact:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the theta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "                \n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of theta from the fit function to each testing datapoint\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.theta @ X[row,]\n",
    "            \n",
    "        return yPredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's write an OLS class using gradient descent instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquaresGradient:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, x0, alpha, h, tolerance, maxIterations):\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        self.h = h\n",
    "        self.alpha = alpha\n",
    "        self.initialGuess = x0\n",
    "        \n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the theta values that minimize the sum of squared errors via gradient descent\n",
    "        X = self.data\n",
    "        L = lambda theta: ((X @ theta).T - y.T) @ (X @ theta - y)\n",
    "        self.theta = gradientDescent(L, self.initialGuess, self.alpha, self.h, tolerance, maxIterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of theta from the fit function to each testing datapoint (rows of X)\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.theta @ X[row,]\n",
    "            \n",
    "        return yPredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Example (Revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE\n",
      "\n",
      "The predicted y values are [1.88927362 2.39728623 2.90529883 3.41331144 2.39728623]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The theta values are [-1.15880201  0.5080126 ]\n",
      "The r^2 score is 0.28901345877326357\n",
      "The mean absolute error is 0.6794572453892237 \n",
      "\n",
      "FOR THE EXACT ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "The predicted y values are [1.84615385 2.38461538 2.92307692 3.46153846 2.38461538]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The theta values are [-1.38461538  0.53846154]\n",
      "The r^2 score is 0.2899408284023671\n",
      "The mean absolute error is 0.6769230769230786\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [7]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE')\n",
    "\n",
    "# instantiate an OLS (gradient) object, fit to data, predict data\n",
    "model = OrdinaryLeastSquaresGradient()\n",
    "model.fit(X, y, [0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('\\nThe predicted y values are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the theta values\n",
    "parameters = model.theta\n",
    "print('The theta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions),'\\n')\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "print('FOR THE EXACT ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS (exact) object, fit to data, predict data\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the theta values\n",
    "parameters = model.theta\n",
    "print('The theta values are', parameters)\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'g', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Example (Revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "The r^2 score is 0.9642661609185357\n",
      "The mean absolute error on the training set is 1.5437720794091632\n",
      "The predicted y values for the test set are [-6.44748058 19.07004759  6.67858093 32.19610911]\n",
      "The real y values for the test set are [ 9 15 25 31]\n",
      "The theta values are [-3.53326263 -6.19573333  9.47724871]\n",
      "The mean absolute error on the test set is 9.758764086775154 \n",
      "\n",
      "FOR THE EXACT ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "The r^2 score is 0.9642679900744417\n",
      "The mean absolute error on the training set is 1.5360000000001235\n",
      "The predicted y values for the test set are [-6.52 19.08  6.6  32.2 ]\n",
      "The real y values for the test set are [ 9 15 25 31]\n",
      "The theta values are [-3.56 -6.24  9.52]\n",
      "The mean absolute error on the test set is 9.799999999999963\n"
     ]
    }
   ],
   "source": [
    "trainX = np.array([[2, 2], [2, 3], [5, 6], [6, 7], [9, 10]])\n",
    "trainY = np.array([3, 13, 19, 29, 35])\n",
    "\n",
    "testX = np.array([[2, 1], [4, 5], [6, 5], [8, 9]])\n",
    "testY = np.array([9, 15, 25, 31])\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresGradient()\n",
    "\n",
    "# fit the model to the training data (find the theta parameters)\n",
    "model.fit(trainX, trainY, [0, 0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are', testY)\n",
    "\n",
    "# print the theta values\n",
    "thetaApprox = model.theta\n",
    "print('The theta values are', thetaApprox)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions), '\\n')\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "print('FOR THE EXACT ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the theta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are', testY)\n",
    "\n",
    "# print the theta values\n",
    "thetaExact = model.theta\n",
    "print('The theta values are', thetaExact)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: High School Graduate Rates in the US (Revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE\n",
      "\n",
      "The r^2 score is 0.40352475296449186\n",
      "The mean absolute error on the training set is 3.6674122809133203\n",
      "The predicted y values for the test set are [82. 80. 89. 85. 81. 81. 85. 90. 78. 84. 83. 80. 83.]\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "The theta values are [82.97247297 -3.76322781  0.79836794 -1.79592293 -1.16423823 -1.61290262\n",
      "  2.114449    0.85703448]\n",
      "The mean absolute error on the test set is 4.004933566373501 \n",
      "\n",
      "FOR THE EXACT ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "The r^2 score is 0.4035248263372673\n",
      "The mean absolute error on the training set is 3.6671426682489634\n",
      "The predicted y values for the test set are [82. 80. 89. 85. 81. 81. 85. 90. 78. 84. 83. 80. 83.]\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "The theta values are [82.97297297 -3.76283254  0.79916919 -1.7971357  -1.16309073 -1.61151368\n",
      "  2.11474925  0.85811261]\n",
      "The mean absolute error on the test set is 4.005110359451168\n"
     ]
    }
   ],
   "source": [
    "# import the data from the csv file to an numpy array\n",
    "data = pd.read_csv('data/US_State_Data.csv', sep=',').to_numpy()\n",
    "#print(data)\n",
    "X = np.array(data[:,1:8], dtype=float)\n",
    "y = np.array(data[:,8], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "# standardize the data\n",
    "trainX = scale(trainX)\n",
    "testX = scale(testX)\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresGradient()\n",
    "\n",
    "# fit the model to the training data (find the theta parameters)\n",
    "model.fit(trainX, trainY, [0, 0, 0, 0, 0, 0, 0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the theta values\n",
    "print('The theta values are', model.theta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions), '\\n')\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print('FOR THE EXACT ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the theta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the theta values\n",
    "print('The theta values are', model.theta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Gradient Descent\n",
    "\n",
    "* We get almost identical results in all of these examples in using a gradient-based method for ordinary least squares.\n",
    "\n",
    "* It runs a little slower, but not much.\n",
    "\n",
    "* We must be careful with the $h$ and tolerance hyperparameters to be sure gradient descent will converge.\n",
    "\n",
    "* Gradient descent in our implementation above does not actually require any derivatives since we only used approximate derivatives.\n",
    "\n",
    "* If we knew formulas for the derivatives, we could compute them exactly to let the step size be exactly proportional to $\\nabla L$. This would drastically reduce the number of times we compute the loss function.\n",
    "\n",
    "* Gradient descent and related methods are the main driver of many machine learning problems that are based on to minimizing a loss function (least squares and neural networks, among others), although we will later need some variants to reduce the computational burden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Kernel Regression/Smoothing\n",
    "\n",
    "Kernel smoothing also worked by creating a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianKernel(x0, x, lam):\n",
    "    return np.exp(-np.linalg.norm(x0 - x) ** 2 / lam ** 2)\n",
    "\n",
    "class KernelRegressionGradient:\n",
    "    def __init__(self, kernel_function, lam, fit_intercept = True):\n",
    "        self.kernel_function = kernel_function\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.lam = lam\n",
    "\n",
    "    def predict(self, x0, X, y, theta0, alpha, h, tolerance, maxIterations):\n",
    "        # find the number of X points\n",
    "        n = X.shape[0]\n",
    "\n",
    "        # add a column of ones if needed\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones([n,1]), X))\n",
    "\n",
    "        # construct the kernel matrix\n",
    "        kernel = np.zeros([n, n])\n",
    "\n",
    "        # populate the kernel matrix\n",
    "        for i in range(n):\n",
    "            kernel[i][i] = self.kernel_function(x0, X[i,:], self.lam)\n",
    "\n",
    "        L = lambda theta: (X @ theta - y).T @ kernel @ (X @ theta - y)\n",
    "        self.theta = gradientDescent(L, theta0, alpha, h, tolerance, maxIterations)\n",
    "\n",
    "        return np.array([1, np.float(x0)]) @ self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the shampoo sales dataset\n",
    "data = pd.read_csv('data/shampoo.csv')\n",
    "\n",
    "# save the targets\n",
    "y = data['Sales'].to_numpy()\n",
    "\n",
    "# make a column vector of 0s with n elements\n",
    "X = np.zeros([y.shape[0], 1])\n",
    "\n",
    "# convert the vector to (0, 1, 2, ..., n)\n",
    "X[:,0] = [i for i in range(y.shape[0])]\n",
    "\n",
    "X = scale(X)\n",
    "\n",
    "# split the data into train and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-c51440236da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# apply the functions to the test data and predict with the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrainPredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mtestPredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-c51440236da1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# apply the functions to the test data and predict with the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrainPredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mtestPredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-7cdb43e88dac>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x0, X, y, theta0, alpha, h, tolerance, maxIterations)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mkernel\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradientDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-7fc2a1f08bbb>\u001b[0m in \u001b[0;36mgradientDescent\u001b[1;34m(f, x0, alpha, h, tolerance, maxIterations)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxIterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# update the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# stop if the norm of the gradient is near 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-7fc2a1f08bbb>\u001b[0m in \u001b[0;36mcomputeGradient\u001b[1;34m(f, x, h)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mxUp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mxUp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mgradient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxUp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lam = 1\n",
    "\n",
    "model = KernelRegressionGradient(GaussianKernel, lam)\n",
    "\n",
    "# compute the model coordinates\n",
    "xModel = scale([i for i in range(20)])\n",
    "yModel = [model.predict(i, trainX, trainY, [0, 0], alpha = 0.001, h = 0.0001, tolerance = 0.01, maxIterations = 100000) for i in xModel]\n",
    "\n",
    "# plot the model\n",
    "label = 'Kernel Smoothing for lambda = ' + str(np.round(lam, 2))\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xModel, yModel, 'r', label = label)\n",
    "plt.legend()\n",
    "    \n",
    "# apply the functions to the test data and predict with the model\n",
    "trainPredictions = [model.predict(i, trainX, trainY, [0, 0], alpha = 0.001, h = 0.0001, tolerance = 0.01, maxIterations = 100000) for i in trainX]\n",
    "testPredictions = [model.predict(i, trainX, trainY, [0, 0], alpha = 0.001, h = 0.0001, tolerance = 0.01, maxIterations = 100000) for i in testX]\n",
    "    \n",
    "# compute the training and test mean absolute error\n",
    "trainError = mean_absolute_error(trainY, trainPredictions)\n",
    "testError = mean_absolute_error(testY, testPredictions)\n",
    "    \n",
    "# return quality metrics\n",
    "print('lambda:', np.round(lam, 3), '\\t\\tr^2:', np.round(r2_score(trainY, trainPredictions), 3),\n",
    "      '\\t\\ttrain MAE:', np.round(trainError, 3), '\\t\\ttest MAE:', np.round(testError, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could run it like last time (below), but it becomes **very** expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamValues = np.linspace(0.025, 2, 5)\n",
    "\n",
    "M = lamValues.shape[0]\n",
    "\n",
    "# allow multiple plots\n",
    "fig, axes = plt.subplots(nrows = M + 1, figsize = (10, 4*M))\n",
    "\n",
    "trainMAE = []\n",
    "testMAE = []\n",
    "\n",
    "for (j, lam) in enumerate(lamValues):\n",
    "    print(j)\n",
    "    model = KernelRegressionGradient(GaussianKernel, lam)\n",
    "\n",
    "    # compute the model coordinates\n",
    "    xModel = scale([i for i in range(20)])\n",
    "    yModel = [model.predict(i, trainX, trainY, [0, 0], alpha = 0.001, h = 0.0001, tolerance = 0.01, maxIterations = 100000)\n",
    "              for i in xModel]\n",
    "\n",
    "    # plot the model\n",
    "    label = 'Kernel Smoothing for lambda = ' + str(np.round(lam, 2))\n",
    "    axes[j].scatter(X, y)\n",
    "    axes[j].plot(xModel, yModel, 'r', label = label)\n",
    "    axes[j].legend()\n",
    "    \n",
    "    # apply the functions to the test data and predict with the model\n",
    "    trainPredictions = [model.predict(i, trainX, trainY, [0, 0], alpha = 0.001, h = 0.0001, tolerance = 0.01, maxIterations = 100000)\n",
    "                        for i in trainX]\n",
    "    testPredictions = [model.predict(i, trainX, trainY, [0, 0], alpha = 0.001, h = 0.0001, tolerance = 0.01, maxIterations = 100000)\n",
    "                       for i in testX]\n",
    "    \n",
    "    # compute the training and test mean absolute error\n",
    "    trainError = mean_absolute_error(trainY, trainPredictions)\n",
    "    testError = mean_absolute_error(testY, testPredictions)\n",
    "    \n",
    "    # save the training and test mean absolute error\n",
    "    trainMAE.append(trainError)\n",
    "    testMAE.append(testError)\n",
    "\n",
    "    # return quality metrics\n",
    "    print('lambda:', np.round(lam, 3), '\\t\\tr^2:', np.round(r2_score(trainY, trainPredictions), 3),\n",
    "          '\\t\\ttrain MAE:', np.round(trainError, 3), '\\t\\ttest MAE:', np.round(testError, 3))\n",
    "    \n",
    "# plot the errors\n",
    "axes[M].plot(range(M), trainMAE, label = 'Training Mean Absolute Error')\n",
    "axes[M].plot(range(M), testMAE, label = 'Testing Mean Absolute Error')\n",
    "axes[M].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 10 - Radial Basis Functions by Gradient Descent\n",
    "\n",
    "Last week, we saw that radial basis function networks model unknown functions as\n",
    "\n",
    "$$f(x)=\\theta_0 + \\sum\\limits_{j=1}^M K_{\\lambda_j}(\\xi_j,x)\\theta_j = \\theta_0+\\sum\\limits_{j=1}^M D\\left(\\frac{\\|\\xi_j-x\\|}{\\lambda_j}\\right)\\theta_j$$\n",
    "\n",
    "where $D$ is some function dependent on the distance between points $\\xi_j$ and $x$, but not otherwise dependent on those variables. The most common is the Gaussian kernel,\n",
    "\n",
    "$$K_{\\lambda_j}(\\xi_j,x) = \\exp\\left(-\\frac{\\|\\xi_j-x\\|}{2\\lambda_j^2}\\right)$$\n",
    "\n",
    "We will try to fit the model to the training data $(x_1,y_1), (x_2, y_2), ..., (x_n, y_n)$, where each point is in $d$-dimensional space: $x_i\\in\\mathbb{R}^d$. The parameter has parameters $\\lambda_j$, $\\theta_j$, $\\xi_j$ for $j=1, 2, ..., M$ and $\\theta_0$, noting each $\\xi_j$ has the same dimension as each $x_i$.\n",
    "\n",
    "A good approach to Fitting the model is to minimize the sum of squared error loss function,\n",
    "\n",
    "$$L(\\theta,\\lambda,\\xi)=\\sum\\limits_{i=1}^n\\left(y_i - \\theta_0 - \\sum\\limits_{j=1}^M\\theta_j\\exp\\left(-\\frac{\\|x_i-\\xi_j\\|^2}{2\\lambda_j^2}\\right)\\right)^2$$\n",
    "\n",
    "That is, to solve the problem\n",
    "\n",
    "$$\\min\\limits_{\\theta, \\lambda, \\xi}L(\\theta,\\lambda,\\xi)$$\n",
    "\n",
    "It turns out, this problem is not particularly easy for a few reasons:\n",
    "\n",
    "* $L$ is non-convex and does not have a unique minimum. There are local minima, so it is difficult to know if we have found the global minimum.\n",
    "* $L$ depends on $M+1$ variables $\\theta_j$, $M$ variables $\\lambda_j$, and $M$ $d$-dimensional variables $\\xi_j$, which makes many variables with respect to which we need to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
